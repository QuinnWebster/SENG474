{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000,)\n",
      "(10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "import repo.utils.mnist_reader as mnist_reader\n",
    "\n",
    "X_train, y_train = mnist_reader.load_mnist('repo/data/fashion', kind='train')\n",
    "X_test, y_test = mnist_reader.load_mnist('repo/data/fashion', kind='t10k')\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 784)\n",
      "(12000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_data_points = len(X_train)\n",
    "\n",
    "X_train_new = []\n",
    "y_train_new = []\n",
    "\n",
    "for i in range(num_data_points):\n",
    "    if y_train[i] == 5:\n",
    "        X_train_new.append(X_train[i])\n",
    "        y_train_new.append(0)\n",
    "    elif y_train[i] == 7:\n",
    "        X_train_new.append(X_train[i])\n",
    "        y_train_new.append(1)\n",
    "\n",
    "\n",
    "\n",
    "X_train_new = np.array(X_train_new)\n",
    "y_train_new = np.array(y_train_new)\n",
    "\n",
    "X_train = X_train_new\n",
    "y_train = y_train_new\n",
    "\n",
    "X_train_og = X_train\n",
    "y_train_og = y_train\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 784)\n",
      "(6000,)\n"
     ]
    }
   ],
   "source": [
    "num_samples_per_case = 3000\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "class_0_indices = np.where(y_train== 0)[0]\n",
    "class_1_indices = np.where(y_train == 1)[0]\n",
    "\n",
    "class_0_indices = np.random.choice(class_0_indices, num_samples_per_case, replace=False)\n",
    "class_1_indices = np.random.choice(class_1_indices, num_samples_per_case, replace=False)\n",
    "\n",
    "X_train_short = np.concatenate([X_train[class_0_indices], X_train[class_1_indices]])\n",
    "y_train_short = np.concatenate([np.zeros(num_samples_per_case), np.ones(num_samples_per_case)])\n",
    "\n",
    "indices = np.random.permutation(len(X_train_short))\n",
    "X_train_short = X_train_short[indices]\n",
    "y_train_short = y_train_short[indices]\n",
    "\n",
    "X_train = X_train_short\n",
    "y_train = y_train_short\n",
    "\n",
    "print(X_train_short.shape)\n",
    "print(y_train_short.shape)\n",
    "\n",
    "# for i in range(6000):\n",
    "    # print(\"Printing\")\n",
    "    # print(y_train_short[i])\n",
    "\n",
    "# for i in range (6000):\n",
    "#     if i == 3000:\n",
    "#         print('halfway')\n",
    "#     print(y_train[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_data_points = len(X_test)\n",
    "\n",
    "X_test_new = []\n",
    "y_test_new = []\n",
    "\n",
    "for i in range(num_data_points):\n",
    "    if y_test[i] == 5:\n",
    "        X_test_new.append(X_test[i])\n",
    "        y_test_new.append(0)\n",
    "    elif y_test[i] == 7:\n",
    "        X_test_new.append(X_test[i])\n",
    "        y_test_new.append(1)\n",
    "\n",
    "X_test_new = np.array(X_test_new)\n",
    "y_test_new = np.array(y_test_new)\n",
    "\n",
    "X_test = X_test_new\n",
    "y_test = y_test_new\n",
    "\n",
    "X_test_og = X_test\n",
    "y_test_og = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assume X_train and X_test are your training and test feature matrices\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training data and transform training data\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform test data using the same scaler (without refitting)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4799 1201\n",
      "0.20016666666666666\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Select a probability in the range from 0.1 to 0.3 (e.g., you might pick 0.1, or 0.2, or 0.25,\n",
    " etc.). Fix this value, and call it p.\n",
    " Now, independently for each example in the training set, flip its label with probability p.\n",
    " This is called adding “label noise”. It makes the labels noisy and makes machine learning\n",
    " algorithms more likely to overfit\"\"\"\n",
    "\n",
    "p = 0.2 # Probabilty\n",
    "\n",
    "def flip_labels(y):\n",
    "    y_noisy = y.copy()\n",
    "    for i in range(len(y)):\n",
    "        random_number = np.random.rand() #Random num between 0 and 1\n",
    "        if random_number < p:\n",
    "            y_noisy[i] = 1 - y_noisy[i]\n",
    "    return y_noisy\n",
    "\n",
    "\n",
    "y_train_noisy = flip_labels(y_train)\n",
    "\n",
    "\n",
    "#Sanity check\n",
    "same = 0\n",
    "diff = 0\n",
    "for i in range(len(y_train)):\n",
    "    if y_train[i] == y_train_noisy[i]:\n",
    "        same += 1\n",
    "    else:\n",
    "        diff += 1\n",
    "\n",
    "print(same, diff)\n",
    "print(diff / len(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 94.50%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "kernal = 'linear'\n",
    "\n",
    "c = 0.0005\n",
    "\n",
    "svm = SVC(kernel=kernal, C=c)\n",
    "\n",
    "svm.fit(X_train, y_train_noisy)\n",
    "\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.45%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "gamma = 0.001\n",
    "c = 1.28\n",
    "\n",
    "kernal = 'rbf'\n",
    "\n",
    "svm = SVC(kernel=kernal, C=c, gamma=gamma)\n",
    "\n",
    "svm.fit(X_train, y_train_noisy)\n",
    "\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 85.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\qwebs\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#Hyperparameters\n",
    "hidden_layer_sizes = 30\n",
    "activation = 'logistic'\n",
    "learning_rate_init = 0.001\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, \n",
    "                    activation=activation,\n",
    "                    learning_rate_init=learning_rate_init)                  \n",
    "\n",
    "mlp.fit(X_train, y_train_noisy)\n",
    "\n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
